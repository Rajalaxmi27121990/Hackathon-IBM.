# -*- coding: utf-8 -*-
"""Machine_Learning_for_Finance_Case_Study (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RosHLMIU_p-DDdZR3u-AaWs--2oZz3Zj
"""

# Import all the necessary libraries

import numpy as np # Mathematical Calculations
import pandas as pd # Data operations and summaries
import seaborn as sns # Good data visualizations
import matplotlib.pyplot as plt # basic Visualizations
import statsmodels.api as sm # Statistical Models
import warnings
warnings.simplefilter(action='ignore')
from scipy import stats
# scikit learn - Machine Learning

pd.set_option('display.max_columns', None)

from google.colab import files
uploaded = files.upload()

college = pd.read_csv("collegeplace.csv")

"""# Preliminary Checks"""

len(college) # Number of Rows in the data

college.shape # Number of rows and columns

college.head(10)# Extract the top n observations from the data

college.tail(10) # Extracts the last n observations

college.sample(10) # Randomly checking some observations in the middle

college.columns # Display the names of the columns
# Advisable to change the names of the columns especially when there is no clarity in the names or when there are spaces in the names

# Renaming the column names appropriately
college.rename(columns={ 'PlacedOrNot':'Target' },inplace=True)# Renames the columns

college.info()# Gives information on Missing values, Data Types
# Check whether any of the columns differ in their data type from what is typically expected

# Changing the data type
college["Target"] = college["Target"].astype("object")

college.describe()# Descriptive Statistics of the columns (Numerical columns)

college.describe(include=['O']) # Basic summary of categorical columns

college.nunique() # Unique number of values in each column

college.skew()#symmetricity of the data

college.kurtosis() # Peaked nature of the data ..The deviation of the data from normality

from statsmodels.stats.descriptivestats import describe
describe(college)

cat_columns = college.select_dtypes(include='object').columns
num_columns = college.select_dtypes(exclude='object').columns

# Display the Frequency of each of the categories in the cateogircal columns
for var in cat_columns:
    print("Name of the Category---------", var)
    print(college[var].value_counts())

# Hashing
college['total_units'][college['total_units']=='3U'] = ">2U"
college['total_units'][college['total_units']=='4U'] = ">2U"

# Errors replacing them with blank values (Data Validation)

college['rate_of_interest'][college['rate_of_interest']<=0] = np.nan
college['income'][college['income'] <= 0] = np.nan
#Winsorization
# Adjust to the minimum or the maximum value

# Any category with less than 25 occurrences, I will replace them with blanks

for var in cat_columns:

    insuf_Values = college[var].value_counts()[college[var].value_counts()<25].reset_index()
    college[var][college[var].isin(insuf_Values["index"])] = np.nan

# Count the number of missing values ine ach column
college.isnull().sum().sort_values(ascending=False)







# Remove the missing rows
college.dropna(axis=0, inplace = True)

cat_columns = college.select_dtypes(include='object').columns
num_columns = college.select_dtypes(exclude='object').columns

# One categorical One numerical column - Descriptive Statistics across the categories
for var in cat_columns:
    print(college.groupby(var).mean())

from itertools import product
cat1 = college[cat_columns]
cat2 = college[cat_columns]
cat_var_prod = list(product(cat1,cat2, repeat = 1))
cat_var_prod

import scipy.stats as ss
result = []
for i in cat_var_prod:
    if i[0] != i[1]:
        result.append((i[0],i[1],list(ss.chi2_contingency(pd.crosstab(
                            college[i[0]], college[i[1]])))[1]))
result
chi_test_output = pd.DataFrame(result, columns = ["var1", "var2", "p-value"])
chi_test_output

# Correlation between two numerical variables
college.corr()

def Zscore_outlier(df):
    out=[]
    m = np.mean(df)
    sd = np.std(df)
    for i in df:
        z = (i-m)/sd
        if np.abs(z) > 4:
            out.append(i)
    print("Outliers:",out)
    print("Number of Outliers:",len(out))

for var in num_columns:
    print(var)
    print(Zscore_outlier(college[var]))

def iqr_outliers(df):
    out=[]
    q1 = df.quantile(0.25)
    q3 = df.quantile(0.75)
    iqr = q3-q1
    Lower_tail = q1 - 3 * iqr
    Upper_tail = q3 + 3 * iqr
    for i in df:
        if i > Upper_tail or i < Lower_tail:
            out.append(i)
    print("Outliers:",out)
    print("Number of Outliers:",len(out))

for var in num_columns:
    print(var)
    print(iqr_outliers(college[var]))

def Winsorization_outliers(df):
    out=[]
    q1 = df.quantile(0.005)
    q3 = df.quantile(0.995)
    for i in df:
        if i > q3 or i < q1:
            out.append(i)
    print(var)
    print("q1:", q1, "q3:",q3)
    print("Outliers:",out)
    print("Number of Outliers:",len(out))

for var in num_columns:
    print(Winsorization_outliers(college[var]))

def ZRscore_outlier(df):
    out=[]
    med = np.median(df)
    ma = stats.median_abs_deviation(df)
    for i in df:
        z = (0.6745*(i-med))/ma
        if np.abs(z) > 4:
            out.append(i)
    print("Outliers:",out)
    print("Number of Outliers:",len(out))

for var in num_columns:
    print(ZRscore_outlier(college[var]))

# Multivariate Outliers

from sklearn.cluster import DBSCAN
def DB_outliers(df):
    outlier_detection = DBSCAN(eps = 2, metric='euclidean', min_samples = 5)
    clusters = outlier_detection.fit_predict(df.values.reshape(-1,1))
    data = pd.DataFrame()
    data['cluster'] = clusters
    print(data['cluster'].value_counts().sort_values(ascending=False))

for var in num_columns:
    print(var)
    print(DB_outliers(college[var]))

# Identifying outliers based on Random Forests

from sklearn.ensemble import IsolationForest
def Iso_outliers(df):
    iso = IsolationForest(random_state = 1, contamination= 'auto')
    preds = iso.fit_predict(df.values.reshape(-1,1))
    data = pd.DataFrame()
    data['cluster'] = preds
    print(data['cluster'].value_counts().sort_values(ascending=False))

for var in num_columns:
    Iso_outliers(college[var])

"""## Visualizations"""

# Numerical Columns - Univariate - Histogram with Density Plot
for var in num_columns:
    plt.figure()
    sns.histplot(data = bank, x = var, kde = True, color='teal', alpha=0.6)
    sns.kdeplot

# Numerical Columns - Univariate - BoxPlot
for var in num_columns:
    plt.figure()
    sns.boxplot(data = bank, x = var, color='teal')

# Categorical Columns - Univariate - Simple Frequency Chart
plt.figure(figsize=(15,8))
for var in cat_columns:
    plt.figure()
    sns.countplot(data = bank, x = var)

# Categorical Columns - Bivariate - Simple Frequency Chart distributed across categories
plt.figure(figsize=(15,8))
for var in cat_columns:
    plt.figure()
    sns.countplot(data = bank, x = "Target", hue = var)

# Numerical - Numerical Scatter Plot - Scatter Plot Matrix # Subdivided by the Target Variable
sns.pairplot(bank)

# Numerical - Numerical Regression Plot -
for var1 in num_columns:
    for var2 in num_columns:
        plt.figure()
        sns.regplot(data = bank, x = var1, y = var2, color='teal')

# Numerical - Numerical Scatter Plot - Scatter Plot Matrix # Subdivided by the Target Variable
sns.pairplot(bank, hue = "Target")

for var in num_columns:
    for i in cat_columns:
        plt.figure()
        sns.catplot(x=i,y=var,data=bank,kind='box',col='Target',palette='Dark2')

# Heatmap representation of correlation
sns.heatmap(bank.corr(), annot=True, cmap = "viridis")

import statsmodels.api as sm
def qq_plots(df):
    plt.figure(figsize=(10, 4))
    sm.qqplot(df,line='s')
    plt.title("Normal QQPlot---"+var)
    plt.show()
for var in num_columns:
    qq_plots(bank[var])

"""## Scikit-Learn"""

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler,PowerTransformer,QuantileTransformer
from sklearn.feature_selection import VarianceThreshold

college['Target'] = college['Target'].replace(['no','yes'],['0','1'])

# Separate the features variables from the Target Variables

X = college.drop(["Target"],axis = 1)
y = college['Target']

from statsmodels.stats.descriptivestats import describe
describe(X)

X[num_columns]

# You can perform Box Cox or Yeo-Johnson (Non Normal transformation)
new_cols = num_columns
bctrans = PowerTransformer(method = 'yeo-johnson').fit(X[num_columns])
X1 = pd.DataFrame(bctrans.transform(X[num_columns]), columns = new_cols)
X1.tail()

X = X.reset_index()

X.tail()

X.drop('index',axis = 1)
X = pd.concat([X1,X[cat_columns.drop('Target')]],axis = 1)

X.tail()

X.shape

X.dropna(inplace = True)

# Have a list of categorical and numerical columns

categorical_columns = X.select_dtypes(include='object').columns
numerical_columns = X.select_dtypes(exclude='object').columns

# Encoding Categorical Variables on the entire Feature data

X = pd.get_dummies(data = X, prefix = categorical_columns, prefix_sep='_',
               columns = categorical_columns,
               drop_first =True,
              dtype='int8')
X.head()

# Splitting the data into training and testing

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Capture the columns of training data

col_names = X_train.columns

X_test.head()

"""## Feature Selection

### Constant Features Removal (Zero Variance Removal)
"""

# Filter out any variables with a single constant value

constant_filter = VarianceThreshold(threshold=0)
constant_filter.fit(X_train)

constant_filter.get_support().sum()

constant_filter.get_support()

col_names = col_names[constant_filter.get_support()]

col_names

X_train = pd.DataFrame(constant_filter.transform(X_train),columns = col_names)
X_test = pd.DataFrame(constant_filter.transform(X_test),columns = col_names)

X_train.head()

"""### Quasi Constant Removal (Near Zero Variance)"""

0.995*0.005

# Set the threshold for Near Zero Variance and run it on the training data

quasi_constant_filter = VarianceThreshold(threshold=0.001)
quasi_constant_filter.fit(X_train)

# Find the number of variables that get retained and collect their column names
quasi_constant_filter.get_support().sum()
col_names = col_names[quasi_constant_filter.get_support()]

quasi_constant_filter.get_support().sum()

quasi_constant_filter.get_support()

# Remove the eliminated columns from both the training and the testing data and retain only the survived columns

X_train = pd.DataFrame(quasi_constant_filter.transform(X_train),columns = col_names)
X_test = pd.DataFrame(quasi_constant_filter.transform(X_test), columns = col_names)

X_train.head()

"""### Remove Duplicate Features"""

# Transpose the rows to columns and vice versa
X_train_T = X_train.T
X_test_T = X_test.T

# Convert the matrix to a Data Frame
X_train_T = pd.DataFrame(X_train_T)
X_test_T = pd.DataFrame(X_test_T)

X_train_T.shape, X_test_T.shape

# Count the number of duplicated rows

X_train_T.duplicated().sum()

# Get the names of duplicated features
duplicated_features = X_train_T.duplicated()
duplicated_features

# Remove the features that are identified as duplicates and retain the remaining ones
features_to_keep = [not index for index in duplicated_features]

features_to_keep

# Gather the names of the retained columns
col_names = col_names[features_to_keep]

# Filter the data by removing the duplicate column
X_train = pd.DataFrame(X_train_T[features_to_keep].T,columns = col_names)
X_test = pd.DataFrame(X_test_T[features_to_keep].T,columns = col_names)

X_train.head()

"""### Correlated Feature Removal"""

# Take paiwise correlation

corrmat = X_train.corr()

# Heat Map of the correlation

plt.figure(figsize=(35,5))
#sns.heatmap(corrmat, annot = True, cmap = 'viridis')
sns.heatmap(corrmat, annot=True)

# Function to capture pair wise correlations greater than a set threshold

def get_correlation(data, threshold):
    corr_col = set()
    corrmat = data.corr()
    for i in range(len(corrmat.columns)):
        for j in range(i):
            if abs(corrmat.iloc[i, j])>= threshold:
                colname = corrmat.columns[i]
                corr_col.add(colname)
    return corr_col

corr_features_1 = get_correlation(X_train, 1)
corr_features_1

col_names

# Simple outright dropping of the correlated columns
X_train = X_train.drop(labels=corr_features_1, axis = 1)
X_test = X_test.drop(labels = corr_features_1, axis = 1)

col_names = col_names.drop(corr_features_1)

X_train.columns = col_names
X_test.columns = col_names

X_train.head()

"""### Feature Grouping and Feature Importance"""

# Take paiwise correlation

corrmat = X_train.corr()

corr_features_1 = get_correlation(X_train, 0.9)
corr_features_1

# Convert the correlation matrix into a columnal form
corrdata = corrmat.abs().stack()
corrdata

corrdata = corrdata.sort_values(ascending=False)
corrdata

corrdata = corrdata[corrdata>=0.9]
corrdata = corrdata[corrdata<1]
corrdata

corrdata

corrdata = pd.DataFrame(corrdata).reset_index()
corrdata.columns = ['features1', 'features2', 'corr_value']
corrdata

# Groups of the correlated features

grouped_feature_list = []
correlated_groups_list = []
for feature in corrdata.features1.unique():
    if feature not in grouped_feature_list:
        correlated_block = corrdata[corrdata.features1 == feature]
        grouped_feature_list = grouped_feature_list + list(correlated_block.features2.unique()) + [feature]
        correlated_groups_list.append(correlated_block)

correlated_groups_list

for group in correlated_groups_list:
    print(group)

# Feature importance using RF classifier

from sklearn.ensemble import RandomForestClassifier
important_features = []
for group in correlated_groups_list:
    features = list(group.features1.unique()) + list(group.features2.unique())
    #features = col_names[features]
    rf = RandomForestClassifier(n_estimators=100, random_state=0)
    y_train=y_train.astype('int')
    rf.fit(X_train[features], y_train)

    importance = pd.concat([pd.Series(features), pd.Series(rf.feature_importances_)], axis = 1)
    importance.columns = ['features', 'importance']
    importance.sort_values(by = 'importance', ascending = False, inplace = True)
    feat = importance.iloc[0]
    important_features.append(feat)

important_features

important_features = pd.DataFrame(important_features)

important_features.reset_index(inplace=True, drop = True)

important_features

features_to_consider = set(important_features)

features_to_discard = set(corr_features_1) - set(features_to_consider)

features_to_discard = list(features_to_discard)

features_to_discard

X_train = X_train.drop(labels = features_to_discard, axis = 1)
X_train.shape

X_test = X_test.drop(labels=features_to_discard, axis = 1)
X_test.shape

X_train.head()

"""## Other univariate feature Selection

### Feature selection based on Chi Square test
"""

#from sklearn.feature_selection import chi2
#from sklearn.feature_selection import SelectKBest, SelectPercentile

#sel = chi2(X_train, y_train)
#sel

#p_values = pd.Series(sel[1])
#p_values.index = X_train.columns
#p_values.sort_values(ascending = True, inplace = True)

#p_values.plot.bar(figsize = (16, 5))

#p_values = p_values[p_values<0.05]

#p_values.index

#X_train = X_train[p_values.index]
#X_test = X_test[p_values.index]

#X_train.head()

#new_cols = X_train_p.columns
#new_cols

"""### Feature selection based on mutual information"""

from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, mutual_info_regression
from sklearn.feature_selection import SelectKBest, SelectPercentile

mi = mutual_info_classif(X_train, y_train.astype(int))

mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False, inplace = True)

mi.plot.bar(figsize = (16,5))

mi

sel = mi[mi.values>0.001].index
sel

X_train = X_train[sel]
X_test = X_test[sel]

X_train.head()

"""## Feature selection based on F Score"""

from sklearn.feature_selection import f_classif, f_regression
from sklearn.feature_selection import SelectKBest, SelectPercentile

sel = f_classif(X_train, y_train)
sel

p_values = pd.Series(sel[1])
p_values.index = X_train.columns
p_values.sort_values(ascending = True, inplace = True)

p_values.plot.bar(figsize = (16, 5))

p_values = p_values[p_values<0.05]

p_values.index

X_train = X_train[p_values.index]
X_test = X_test[p_values.index]

X_train.head()

new_cols = X_train.columns
new_cols

"""### using PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=42)
pca.fit(X_train)

X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
X_train_pca.shape, X_test_pca.shape

X_train_pca

scaler = preprocessing.StandardScaler().fit(X_train)
X_train = pd.DataFrame(scaler.transform(X_train),columns = new_cols)
X_test = pd.DataFrame(scaler.transform(X_test),columns = new_cols)
X_train.head()

y_train = y_train.astype('int')
y_test = y_test.astype('int')

"""## Basic Perceptron"""

from sklearn.linear_model import Perceptron
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Execute the Default model
model = Perceptron()
model.fit(X_train,y_train)
model.coef_ # Only for a few models

# Accuracy on training data
model.score(X_train,y_train)

y_train.value_counts()

2375/(2375+453)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
#y_pred_proba = model.predict_proba(X_test)

pd.DataFrame(y_test).value_counts()

X_test

1033/(1033+179)

# Print Some importance performance measures
print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(cohen_kappa_score(y_test,y_pred))
#print(roc_auc_score(y_test, y_pred_proba[:,1]))
#plot_roc_curve(model,X_test,y_test)

950/1033

1033*1111/1212

179*101/1212

(946.9166666666666+14.916666666666666)/1212

(0.7986798679867987-0.7935918591859186)/(1-0.7935918591859186)

"""## Logistic Regression"""

t = np.linspace(-100, 100, 1000)
sig = 1 / (1 + np.exp(-t))
plt.figure(figsize=(9, 3))
plt.plot(t, sig, "b-", linewidth=2, label=r"$\sigma(t) = \frac{1}{1 + e^{-t}}$")
plt.xlabel("t")
plt.legend(loc="upper left", fontsize=20)
plt.axis([-10, 10, -0.1, 1.1])
plt.show()

from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

model = LogisticRegression()
model.fit(X_train,y_train)

X_train.columns

# Accuracy on training data
model.score(X_train,y_train)

pd.DataFrame(y_train).value_counts()

4079/(4079+3731)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
y_pred_proba = model.predict_proba(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))

# Cross Validation as a better process over train-test split
from sklearn.model_selection import cross_val_score
scores_accuracy = cross_val_score(model, X, y, cv=10, scoring='accuracy')
scores_balanced_accuracy = cross_val_score(model, X, y, cv=10, scoring='balanced_accuracy')
scores_auc = cross_val_score(model, X, y, cv=10, scoring='roc_auc')
scores_accuracy.mean(),scores_balanced_accuracy.mean(), scores_auc.mean()

scores_accuracy,scores_balanced_accuracy,scores_auc

# Hyper parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
param_grid = {'penalty':['l1','l2','elasticnet','none'],
              'C':[0.01,0.1,1,10,100],
              'fit_intercept':[True, False]}

# Initiate your model
model = LogisticRegression()
# Get a list of all possible parameters

grid_search_acc = GridSearchCV(estimator = model,
                           param_grid = param_grid,
                           scoring = 'accuracy',
                           cv = 10,
                           verbose=0)
grid_search_Bal_acc = GridSearchCV(estimator = model,
                           param_grid = param_grid,
                           scoring = 'balanced_accuracy',
                           cv = 10,
                           verbose=0)
grid_search_auc = GridSearchCV(estimator = model,
                           param_grid = param_grid,
                           scoring = 'roc_auc',
                           cv = 10,
                           verbose=0)
grid_search_acc.fit(X_train, y_train)
grid_search_Bal_acc.fit(X_train, y_train)
grid_search_auc.fit(X_train, y_train)

print(grid_search_acc.best_params_)
print(grid_search_Bal_acc.best_params_)
print(grid_search_auc.best_params_)

grid_search_acc = LogisticRegression(C = 1, fit_intercept= False, penalty= 'l2')
grid_search_acc.fit(X_train,y_train)

y_pred = grid_search_acc.predict(X_test)

grid_search_acc

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))

import statsmodels.api as sm

y_train

"""## KNN Classifier"""

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier()
model.fit(X_train,y_train)

# Accuracy on training data
model.score(X_train,y_train)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
y_pred_proba = model.predict_proba(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
plot_confusion_matrix(model,X_test,y_test)
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))
plot_roc_curve(model,X_test,y_test)

# Hyper parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
param_grid = {'n_neighbors':[3,5,9,15,25],
              'weights':['uniform','distance'],
              'p':[1,2,3]
              }

# Initiate your model
model = KNeighborsClassifier()
# Get a list of all possible parameters

grid_search_acc = GridSearchCV(estimator = model,
                           param_grid = param_grid,
                           scoring = 'accuracy',
                           cv = 10,
                           verbose=0)
grid_search_Bal_acc = GridSearchCV(estimator = model,
                           param_grid = param_grid,
                           scoring = 'balanced_accuracy',
                           cv = 10,
                           verbose=0)
grid_search_auc = GridSearchCV(estimator = model,
                           param_grid = param_grid,
                           scoring = 'roc_auc',
                           cv = 10,
                           verbose=0)
grid_search_acc.fit(X_train, y_train)
grid_search_Bal_acc.fit(X_train, y_train)
grid_search_auc.fit(X_train, y_train)

print(grid_search_acc.best_params_)
print(grid_search_Bal_acc.best_params_)
print(grid_search_auc.best_params_)

y_pred = grid_search_auc.predict(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))

np.round(np.sqrt(X_train.shape[0]),0)

neighbors = np.arange(1, int(np.round(np.sqrt(X_train.shape[0]),0)))
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))


for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    train_accuracy[i] = knn.score(X_train, y_train)
    test_accuracy[i] = knn.score(X_test, y_test)

    # Generate plot
plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')

plt.legend()
plt.xlabel('n_neighbors')
plt.ylabel('Accuracy')
plt.show()
test_accuracy

"""# Support Vector Machines"""

from sklearn.svm import SVC

model = SVC(probability = True)
model.fit(X_train,y_train)

# Accuracy on training data
model.score(X_train,y_train)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
y_pred_proba = model.predict_proba(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
plot_confusion_matrix(model,X_test,y_test)
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))
plot_roc_curve(model,X_test,y_test)

# Cross Validation as a better process over train-test split
from sklearn.model_selection import cross_val_score
scores_accuracy = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')
scores_balanced_accuracy = cross_val_score(model, X_train, y_train, cv=10, scoring='balanced_accuracy')
scores_auc = cross_val_score(model, X_train, y_train, cv=10, scoring='roc_auc')
scores_accuracy.mean(),scores_balanced_accuracy.mean(), scores_auc.mean()

scores_accuracy,scores_balanced_accuracy,scores_auc

# Hyper parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
param_grid = {'C':[0.01,0.1,1,10,100],
              'kernel':['linear','rbf','sigmoid'],
              'gamma':['auto',0.01, 0.05,0.1]
              }

# Initiate your model
model = SVC()
# Get a list of all possible parameters

grid_search_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'accuracy',
                           cv = 10,
                           verbose=0)
grid_search_Bal_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'balanced_accuracy',
                           cv = 10,
                           verbose=0)
grid_search_auc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'roc_auc',
                           cv = 10,
                           verbose=0)
grid_search_acc.fit(X_train, y_train)
grid_search_Bal_acc.fit(X_train, y_train)
grid_search_auc.fit(X_train, y_train)

print(grid_search_acc.best_params_)
print(grid_search_Bal_acc.best_params_)
print(grid_search_auc.best_params_)

y_pred = grid_search_acc.predict(X_test)

grid_search_acc

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))

"""# Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train,y_train)

# Accuracy on training data
model.score(X_train,y_train)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
y_pred_proba = model.predict_proba(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
plot_confusion_matrix(model,X_test,y_test)
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))
plot_roc_curve(model,X_test,y_test)

# Hyper parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
param_grid = {'criterion':['gini','entropy','log_loss'],
              'max_depth':[2,3,4,5,"None"],
              'min_samples_split':[2,3,5,8],
              'min_samples_leaf':[1,3,5]}

# Initiate your model
model = DecisionTreeClassifier()
# Get a list of all possible parameters

grid_search_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'accuracy',
                           cv = 10,
                           verbose=0)
grid_search_Bal_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'balanced_accuracy',
                           cv = 10,
                           verbose=0)
grid_search_auc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'roc_auc',
                           cv = 10,
                           verbose=0)
grid_search_acc.fit(X_train, y_train)
grid_search_Bal_acc.fit(X_train, y_train)
grid_search_auc.fit(X_train, y_train)

print(grid_search_acc.best_params_)
print(grid_search_Bal_acc.best_params_)
print(grid_search_auc.best_params_)

y_pred = grid_search_acc.predict(X_test)

grid_search_acc

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))



"""# Random Forest Classification"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train,y_train)

# Accuracy on training data
model.score(X_train,y_train)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
y_pred_proba = model.predict_proba(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
plot_confusion_matrix(model,X_test,y_test)
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))
plot_roc_curve(model,X_test,y_test)

# Hyper parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
param_grid = {'criterion':['gini','entropy'],
              'max_depth':['None',2,3,5],
              'max_features':['auto',2,3,5],
              'min_samples_split':[2,3,5],
              'n_estimators': [50,100,200]
              }

# Initiate your model
model = RandomForestClassifier()
# Get a list of all possible parameters

grid_search_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'accuracy',
                           cv = 10,
                           verbose=0)
grid_search_Bal_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'balanced_accuracy',
                           cv = 10,
                           verbose=0)
grid_search_auc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'roc_auc',
                           cv = 10,
                           verbose=0)
grid_search_acc.fit(X_train, y_train)
grid_search_Bal_acc.fit(X_train, y_train)
grid_search_auc.fit(X_train, y_train)

print(grid_search_acc.best_params_)
print(grid_search_Bal_acc.best_params_)
print(grid_search_auc.best_params_)

y_pred = grid_search_acc.predict(X_test)

grid_search_acc

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))

### Visualize decision Trees

model = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, max_features= 8, min_samples_split= 2)
model.fit(X_train,y_train)
from sklearn import tree
text_representation = tree.export_text(model)
print(text_representation)

fig = plt.figure(figsize=(25,20))
_ = tree.plot_tree(model,
                   feature_names=X_train.columns,
                   filled=True)

fig.savefig("decistion_tree.png")

"""# Gradient Booosting Classification"""

from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier()
model.fit(X_train,y_train)

# Accuracy on training data
model.score(X_train,y_train)

# Predict the model on the Testing Data
y_pred = model.predict(X_test)
# Predict probabilities of belonging to each of the groups
y_pred_proba = model.predict_proba(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
plot_confusion_matrix(model,X_test,y_test)
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))
plot_roc_curve(model,X_test,y_test)

scores_accuracy,scores_balanced_accuracy,scores_auc

# Hyper parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
param_grid = {'max_depth':['None',2,3,4,5],
              'max_features':['auto',2,3,5,8],
              'min_samples_split':[2,3,5,8],
              'n_estimators': [25,50,100,200,400],
               'learning_rate': [0.01,0.05,0.1,0.2,0.5]
              }

# Initiate your model
model = GradientBoostingClassifier()
# Get a list of all possible parameters

grid_search_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'accuracy',
                           cv = 10,
                           verbose=0)
grid_search_Bal_acc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'balanced_accuracy',
                           cv = 10,
                           verbose=0)
grid_search_auc = RandomizedSearchCV(estimator = model,
                           param_distributions = param_grid,
                           scoring = 'roc_auc',
                           cv = 10,
                           verbose=0)
grid_search_acc.fit(X_train, y_train)
grid_search_Bal_acc.fit(X_train, y_train)
grid_search_auc.fit(X_train, y_train)

print(grid_search_acc.best_params_)
print(grid_search_Bal_acc.best_params_)
print(grid_search_auc.best_params_)

y_pred = grid_search_acc.predict(X_test)

grid_search_acc

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(roc_auc_score(y_test, y_pred_proba[:,1]))



"""## XGBOOST"""

from xgboost import XGBClassifier

model = XGBClassifier(use_label_encoder=False,
                      booster='gbtree', # boosting algorithm to use, default gbtree, othera: gblinear, dart
                      n_estimators=100, # number of trees, default = 100
                      eta=0.3, # this is learning rate, default = 0.3
                      max_depth=6, # maximum depth of the tree, default = 6
                      gamma = 0, # used for pruning, if gain < gamma the branch will be pruned, default = 0
                      reg_lambda = 1, # regularization parameter, defautl = 1
                      #min_child_weight=0 # this refers to Cover which is also responsible for pruning if not set to 0
                     )

clf = model.fit(X_train, y_train)

# Predict class labels on training data
pred_labels_tr = model.predict(X_train)
# Predict class labels on a test data
pred_labels_te = model.predict(X_test)

score_te = model.score(X_test, y_test)
score_te

score_tr = model.score(X_train, y_train)
score_tr

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

param_grid = {'eta':[0,0.2,0.4,0.6,0.8,1],'gamma':[0,1,2,4,8,16],"n_estimators":[1,5,10,20,40,100],'min_child_weight':[0,1,2,4,8,16],'max_depth':range(2,10),'subsample':[0,0.2,0.4,0.6,0.8,1]}

grid = RandomizedSearchCV(model,param_distributions=param_grid,scoring = 'accuracy',n_iter = 50)
grid.fit(X_train,y_train)
grid.best_params_

y_pred = grid.predict(X_test)
[round(accuracy_score(y_test,y_pred),3),round(cohen_kappa_score(y_test,y_pred),3),round(roc_auc_score(y_test,y_pred),3)]

"""## Naive bayes"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

print(accuracy_score(y_test,y_pred))
print(balanced_accuracy_score(y_test,y_pred))
print(f1_score(y_test,y_pred))
print(precision_score(y_test,y_pred))
print(recall_score(y_test,y_pred))
plot_confusion_matrix(classifier,X_test,y_test)
print(cohen_kappa_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
#print(roc_auc_score(y_test, y_pred_proba[:,1]))
plot_roc_curve(classifier,X_test,y_test)

accuracy_score(y_test, y_pred)

"""## MultiLayer Perceptron (Artificial Neural Network)"""

from sklearn.neural_network import MLPClassifier

model = MLPClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

plot_confusion_matrix(model,X_test,y_test)

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

param_grid = {'activation':['identity','logistic','tanh','relu'],
             'learning_rate':['constant','invscaling','adaptive'],
              'learning_rate_init':[0.001, 0.005, 0.01],
              'shuffle':[True, False]}

grid = RandomizedSearchCV(model,param_distributions=param_grid)
grid.fit(X_train,y_train)
grid.best_params_

y_pred = grid.predict(X_test)
[round(accuracy_score(y_test,y_pred),3),round(cohen_kappa_score(y_test,y_pred),3),round(roc_auc_score(y_test,y_pred),3)]

"""# Implementing an end-end project using Pipeline"""

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import  MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import cohen_kappa_score
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

def GetBasedModel():
    basedModels = []
    basedModels.append(('LR'   , LogisticRegression()))
    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))
    basedModels.append(('KNN'  , KNeighborsClassifier()))
    basedModels.append(('CART' , DecisionTreeClassifier()))
    basedModels.append(('NB'   , GaussianNB()))
    basedModels.append(('SVM'  , SVC(probability=True)))
    basedModels.append(('AB'   , AdaBoostClassifier()))
    basedModels.append(('GBM'  , GradientBoostingClassifier()))
    basedModels.append(('RF'   , RandomForestClassifier()))
    basedModels.append(('NN'   , MLPClassifier()))

    return basedModels

def BasedLine2(X_train, y_train,models):
    # Test options and evaluation metric
    num_folds = 10
    scoring = 'accuracy'

    results = []
    names = []
    for name, model in models:
        kfold = StratifiedKFold(n_splits=num_folds)
        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
        print(msg)

    return names, results

models = GetBasedModel()
names,results = BasedLine2(X_train, y_train,models)

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler


def GetScaledModel(nameOfScaler):

    if nameOfScaler == 'standard':
        scaler = StandardScaler()
    elif nameOfScaler =='minmax':
        scaler = MinMaxScaler()

    pipelines = []
    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))
    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))
    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))
    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))
    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))
    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))
    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))
    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))
    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))
    pipelines.append((nameOfScaler+'NN'  , Pipeline([('Scaler', scaler),('NN'  , MLPClassifier())])  ))


    return pipelines

models = GetScaledModel('minmax')
names,results = BasedLine2(X_train, y_train,models)

